name: Deploy Infrastructure

on:
  workflow_dispatch:

jobs:
  deploy-infra:
    runs-on: [self-hosted, eks]
    # runs-on: ubuntu-latest #change to self-hosted when using runner runs-on: [self-hosted, bankingKuber]
    permissions:
      id-token: write
      contents: read
    env:
      AWS_ACCOUNT_ID: 961477247679
      AWS_REGION: eu-central-1
      EKS_CLUSTER_NAME: Banking-Kube-Sloth
    steps:
      #public hosted runner
      # - name: Checkout Repository
      #   uses: actions/checkout@v3 for public runner

      #needed for self-hosted runner 
      - name: Install Docker
        run: |
          sudo yum install -y docker
          sudo systemctl start docker
          sudo systemctl enable docker
          sudo usermod -aG docker $USER
          docker --version
          sudo chmod 666 /var/run/docker.sock
      #needed for self-hosted runner 
      - name: Install Git
        run: |
          sudo yum install -y git
          git --version

      #needed for self-hosted runner 
      - name: Manual Git Checkout
        run: |
          cd /home/ssm-user/
          sudo rm -rf EKS-BankingKube # Cleanup before checkout
          git clone https://github.com/Droshow/EKS-BankingKube.git
          cd EKS-BankingKube
          git fetch --tags
          git checkout ${{ github.sha }}
          ls -la
      
      # - name: Run Node.js in Docker
      #   working-directory: /home/ssm-user/EKS-BankingKube
      #   run: |
      #     docker run --rm -v $(pwd):/workspace -w /workspace node:18-alpine node -v
      #     docker run --rm -v $(pwd):/workspace -w /workspace node:18-alpine npm install
      #     docker run --rm -v $(pwd):/workspace -w /workspace node:18-alpine npm run build

           # Step 2: Debug OIDC Setup
      - name: Debug OIDC Setup
        run: |
          echo "Debugging OIDC setup..."
          echo "GitHub Repository: ${{ github.repository }}"
          echo "GitHub Ref: ${{ github.ref }}"
          echo "GitHub Workflow: ${{ github.workflow }}"
          echo "GitHub Run ID: ${{ github.run_id }}"
          echo "OIDC permissions are enabled with id-token: write."

        #public-runner
      # - name: Configure AWS Credentials
      #   id: configure-aws-credentials
      #   uses: aws-actions/configure-aws-credentials@v2
      #   with:
      #     role-to-assume: arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/ci-cd-role
      #     aws-region: eu-central-1
      - name: Assume Role Manually
        run: |
          ROLE_ARN="arn:aws:iam::${{ env.AWS_ACCOUNT_ID }}:role/ci-cd-role"
          SESSION_NAME="ci-cd-session"

          # Assume the role and extract temporary credentials
          CREDS=$(aws sts assume-role --role-arn "$ROLE_ARN" --role-session-name "$SESSION_NAME" --output json)
          export AWS_ACCESS_KEY_ID=$(echo $CREDS | jq -r '.Credentials.AccessKeyId')
          export AWS_SECRET_ACCESS_KEY=$(echo $CREDS | jq -r '.Credentials.SecretAccessKey')
          export AWS_SESSION_TOKEN=$(echo $CREDS | jq -r '.Credentials.SessionToken')

          echo "AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY" >> $GITHUB_ENV
          echo "AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN" >> $GITHUB_ENV

          # Confirm the credentials are set
          echo "Assumed role and set temporary credentials."
          aws sts get-caller-identity
      # public runner
      # - name: Set Up Terraform
      #   uses: hashicorp/setup-terraform@v2
      #   with:
      #     terraform_version: 1.5.7
      - name: Install Terraform Manually
        run: |
          TERRAFORM_VERSION="1.5.7"
          curl -O https://releases.hashicorp.com/terraform/${TERRAFORM_VERSION}/terraform_${TERRAFORM_VERSION}_linux_amd64.zip
          unzip terraform_${TERRAFORM_VERSION}_linux_amd64.zip
          sudo mv terraform /usr/local/bin/
          terraform -version

      - name: Print working directory
        run: pwd

      - name: Initialize Terraform
        working-directory: /home/ssm-user/EKS-BankingKube/EKS_infra #EKS_infra for public runner
        run: terraform init
      
      # - name: state import
      #   working-directory: EKS_infra
      #   run: |
      #     terraform import -var-file="envs/terraform_dev.tfvars" aws_iam_role.ci_cd_role ci-cd-role
      #     terraform import -var-file="envs/terraform_dev.tfvars" aws_iam_policy.ci_cd_custom_policy arn:aws:iam::961477247679:policy/ci-cd-custom-policy
      #     terraform import -var-file="envs/terraform_dev.tfvars" aws_iam_openid_connect_provider.github arn:aws:iam::961477247679:oidc-provider/token.actions.githubusercontent.com
      - name: Plan Terraform
        working-directory: /home/ssm-user/EKS-BankingKube/EKS_infra 
        run: terraform plan -var-file="envs/terraform_dev.tfvars"
#temp off
      - name: Apply Terraform
        working-directory: /home/ssm-user/EKS-BankingKube/EKS_infra

        #commenting out general terraform apply to test just SSM functionality 
        # run: terraform apply -auto-approve -var-file="envs/terraform_dev.tfvars"
        run: | 
          terraform apply -auto-approve -var-file="envs/terraform_dev.tfvars" \
              --target=module.networking \
              --target=module.security \
              --target=module.ec2_cluster_access \
              --target=module.eks \
              --target=module.ecr \
              --target=module.node_groups \
              --target=module.storage

      
      # - name: Update Kubeconfig
      #   run: |
      #     mkdir -p ~/.kube
      #     aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}
      # - name: Verify kubectl Access
      #   run: |
      #     kubectl get nodes
      